{
  "hash": "9cd94b0122562535cf85c1db31f03cc6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Integrating Large Language Model tools to our R workflows\"\nsubtitle: \"Physalia Courses\"\ndate: \"November 5 2025\"\nauthor: \"Luis D. Verde Arregoitia\"\nformat:\n  revealjs:\n    slide-number: true\n    theme: slides.scss\neditor: visual\n---\n\n## Background\n\nLanguage Models?\n<br/>\n\n. . .\n\n**Large** Language Models?\n<br/>\n\n. . .\n\nGenerative AI?\n\n## \n\n<br/><br/>\n\n:::::: columns\n::: {.column width=\"30%\"}\n![AI-generated image created with DALL¬∑E via ChatGPT](imgs/mami.png)\n:::\n\n:::: {.column width=\"70%\"}\n::: large-font-slide\nAfter drinking some water, the dog went back to the bed to feed her \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_.\n:::\n::::\n::::::\n\n. . .\n\n## Token probabilities\n\nLet's use `promptr` to explore the probabilistic nature of LLMs (and check if our OpenAI key is set up correctly)\n\n<br/>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(promptr)\npromptr::complete_prompt(\"After drinking some water, the dog went to bed feed her\")\n```\n:::\n\n\n<br/>\n-   Let's change the prompt and the `max_tokens` argument\n\n##  Demo\n\nToken Possibilities app by Garrick Aiden Buie\n\n## \n\n### Language models\n\nPredict and generate text by estimating the probability of tokens occurring in a given sequence.\n\n<br/>\n\n### *Large* language models\n\nAdd more parameters and massive datasets, enabling more complex tasks\n\n<br/>\n\n#### Tokens\n\nBasic units of text that LLMs use to process language. May be words, character sets, combinations of words, and punctuation.\n\n## \n\nJim Amos on linkedin:\n\n> LLMs are language calculators: they use Natural Language Processing algorithms to intepret prompts then generate statistically plausible answers rendered from algebraic vectors -- afterimages of training data compressed into non-euclidian space...\n\n. . .\n\n<br/>\n\n**statistically plausible** can be good enough for what we need\n\n\n## LLMs can help us with:\n\n::: large-font-slide\nText generation, translation, sentiment analysis, writing and debugging code, generating images and videos, etc.\n:::\n\n# However\n\n## Let's consider:\n\n-   Growing evidence of negative effects on learning\n-   Biases\n-   Hype\n-   Valid anti-AI sentiment\n-   Issues with training material\n-   Provider intentions\n-   Lax security\n\n## üö´ Hype üö´\n\n\n::::: columns\n::: {.column width=\"40%\"}\n![](imgs/onvres.jpeg)\n:::\n\n::: {.column width=\"60%\"}\n\"If you don't become an AI/prompt engineer by tomorrow you're a loser with no future\"\n\n<br/> \"Your product needs AI or it will be worthless\"\n\n<br/> \"All your training in data and coding was a waste of time\"\n:::\n:::::\n\n## Still...\n\n> I believe everyone deserves the ability to automate tedious tasks in their lives with computers - Simon Willison\n\n> ..providing a way for people to talk with machines in plain language constitutes a dramatic step forward in making computing accessible to everyone - Carl Bergstrom (paraphrased)\n\n## Where are the models? Who makes them?\n\nMany popular models live on the cloud\n\n-   [OpenAI](https://openai.com/) (Known for GPT series including ChatGPT)\n-   [Google AI](https://ai.google.dev/) (Gemini models)\n-   [Anthropic](https://www.anthropic.com/) (Claude models)\n-   [DeepSeek](https://www.deepseek.com/en) (DeepSeek R1)\n- [GitHub Copilot](https://github.com/copilot) (multiple models)\n\n\n## Models != Providers\n\nCompanies train and release models for different use cases\n\n:::{.incremental}\n- General purpose, lightweight models, coding, etc.\n- Models may be open or proprietary\n- Cloud providers may host multiple models from different makers\n- Web-based chats include models + various tools (which are not necessarily evident)\n:::\n## Let's explore\n\n::: large-font-slide\n- [Models](https://docs.continue.dev/customize/model-providers) supported by Continue\n\n- LLM [leaderboard](https://artificialanalysis.ai/leaderboards/models)\n:::\n\n## Cloud-Hosted models\n\n:::::: columns\n::: {.column width=\"43%\"}\n-   Relatively easy setup\n-   Create account, set up billing if applicable, get an API key\n-   Access to new and in-development models and massive computing power\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"43%\"}\n-   Can be costly\n\n-   Need internet access\n\n-   We send our data to the API\n:::\n::::::\n\n## Local models\n\nDownload smaller models on our own hardware\n\n:::::: columns\n::: {.column width=\"43%\"}\n-   Data not shared with a provider\n-   No ongoing costs\n-   Work offline\n:::\n\n::: {.column width=\"4%\"}\n:::\n\n::: {.column width=\"43%\"}\n-   Steep learning curve\n-   Large memory requirements\n-   Slower performance\n:::\n::::::\n\n<br/>\n\n#### Examples:\n\n[`Ollama`](https://ollama.com/) (Simplifies running open-source LLMs locally)\\\n[Hugging Face `transformers`](https://huggingface.co/docs/transformers/index) (lots of open-source models)\n\n## Which model do I use?\n\n### Considerations\n\n::: columns\n::: {.column width=\"50%\"}\n-   Pricing\n-   Free tiers, token pricing, billing policies\n-   Context Windows\n-   **Compatibility with our chosen tools**\n-   Speed\n\n:::\n\n::: {.column width=\"50%\"}\n-   Performance\n-   Privacy\n-   Hardware\n:::\n:::\n\n\n## On pricing\n\n> Nothing is more costly than something given free of charge\\\n> - Japanese proverb.\n\n. . .\n\n-   nothing is offered for free unless there is something to be gained for the party involved\n\n-   if we are not paying for the product, then **we** are the product\n\n# Why LLMs + R\n\n## Decent enough outputs\n\n- Working code -> actual problems solved & time saved\n\n- Updated and more recent knowledge cut-offs\n\n## Explosion of programs, packages and features since late 2024\n\n- Relevant keynotes, blog posts, demos, workshops\n\n- üí°[R + AI conference](https://rconsortium.github.io/RplusAI_website/) happening next week!\n\n## Cut down on **context switching**\n\n- Growing support in R for AI tools\n\nShifting our attention between different tasks or programs can be tiring and make us less productive and efficient.\n\n:::{.incremental}\n- Ask the chatbot in the browser\n- Copy the response and paste it in R\n- Share the errors or output with the chat\n- Repeat (and potentially introduce errors)\n:::\n\n# Getting started\n\n## Coding with LLMs\n<br/>\n\n### Autocomplete\n\n<br/>\n\n-   Models suggest 'ghost text' as we type  \n-   Very distracting during teaching  \n-   Suggestions may be unhelpful but mostly harmless\n\n## Coding with LLMs\n\n<br/>\n\n### Chats\n\n<br/>\n\n-   Send prompts, get answers\n-   Often let us provide context and upload files\n-   Results can be copied or inserted directly to a script\n-   In the browser or through APIs \n\n## Coding with LLMs\n<br/>\n\n### Agents/Agentic\n\n:::{.incremental}\n-   Can run code and change files\n-   Follow a general plan and iterates\n-   Works on a task 'autonomously'\n-   Token-hungry and potentially slow\n-   Makes us assume that results are correct\n-   Game-changing potential\n:::\n\n# Conversations with LLMs\n\n## Questions we might put in an LLM chat window?\n\n<br/>\n\n::: retro-console-window\n\\> How do I add a subtitle to my ggplot figure?\\\n\\> What are the arguments for pivot_wider()?\\\n\\> I can't join my table_1 object with my dat3 data frame, help!\n:::\n\n<br/>\n\n. . . \n\n\nNote the difference in the type of question. This will be important soon.\n\n## Request-response cycle\n\nDifferent parts of the internet communicate through HTTP requests. This includes interacting with LLMs.\n\n- Web searches\n- File downloads\n- Posting a status on social media\n\n\n::: {.cell}\n\n:::\n\n\n## \n\n| Role | Description |\n|:----------------|:------------------------------------------------------|\n| `system_prompt` | Overarching instructions from that define the behavior of the assistant |\n| `user` | Messages from the user interacting<br>with the model |\n| `assistant` | The model's responses to the user |\n\n## Conversation with a chatbot\n\n<br> üßç: name three mammals <br> \nü§ñ: dog, cat, mouse\n<br/>\n\n<hr>\n\nüßç: name three mammals <br> \nü§ñ: perro, gato, rat√≥n\n\n. . . \n\n<br/>\n<br/>\n**What happened?**\n\n## System Prompts\n\nDefine specific instructions on role, tone, or constraints\n\n<br/>\n\n### No system prompt?\n\n-   Models default to base programming, leads to general and less predictable responses\n\n\n## System Prompts\n\nRepeatable, consistent results suited to what we need to do.\n\n- \"you are an expert R programmer that only uses base R\"\n\n- \"you are an expert R programmer that only uses base R. Do not add any comments. Do not explain the output\"\n\n- \"you are an expert R programmer. Comment the code in French\"\n\n::::{.rightref}\n:::{.refbox}\nellmer prompt design [Vignette](https://ellmer.tidyverse.org/articles/prompt-design.html)\n:::\n::::  \n\n## LLMs are stateless\n\n![](imgs/MementO.jpg)\n\n## LLMs are stateless\n\n-   LLMs don't retain memory of prior interactions\n\n-   Context happens by sending all previous messages with each new request\n\n-   Because of limitations on tokens and context size, there are truncation and optimization strategies at work\n\n## LLMs are stateless\n\n:::{large-slide-text}\n\n- Start new chats often\n\n- Split up tasks across chats\n\n- Use detailed system prompts\n\n:::\n\n# Break",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}